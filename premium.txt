!apt update && apt install -y curl git


!curl -fsSL https://ollama.com/install.sh | sh


OLLAMA_API_URL = "http://127.0.0.1:11434"
!nohup ollama serve > /dev/null 2>&1 &


!ollama pull llama3.2


!pip install openai

!pip install gradio

!pip install PyMuPDF

!pip install faiss-cpu

!ollama pull nomic-embed-text

!pip uninstall pandas -y
!pip install pandas --upgrade

!pip install git+https://github.com/openai/whisper.git

!apt-get install -y ffmpeg


!pip install googletrans

!apt-get install -y libsndfile1
!pip install soundfile


!pip install gtts

import os
from getpass import getpass

# Ask for the GitHub token securely
GITHUB_TOKEN = getpass("Enter your GitHub Token: ")
os.environ["GITHUB_TOKEN"] = GITHUB_TOKEN


import gradio as gr
import asyncio
import uuid
import numpy as np
import faiss
from openai import OpenAI
from gtts import gTTS
import whisper
from googletrans import Translator
import pandas as pd
import fitz

import os

# GitHub-hosted OpenAI-compatible models
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
BASE_URL = "https://models.github.ai/inference"

AVAILABLE_MODELS = [
    "openai/gpt-4.1",
    "openai/gpt-4.1-mini",
    "openai/gpt-4o",
    "openai/gpt-4o-mini",
    "openai/gpt-4.1-nano"
]

# Initialize OpenAI API and Whisper Model
openai = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")
whisper_model = whisper.load_model("tiny")

# Initialize TTS for multilingual support (Load default English model)


# Placeholder for your RAG data
rag_df = pd.read_csv('/content/updated_disease_data_13_fuzzy_filled (1).csv')
rag_df2 = pd.read_csv('/content/serious_diseases.csv')
rag_df3 = pd.read_csv('/content/dis_descp.csv')

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        return "\n".join(page.get_text() for page in doc)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""

cure_text = extract_text_from_pdf('/content/TheCureForAllDiseases.pdf')
textbook_text = extract_text_from_pdf('/content/Harrison.pdf')

rag_documents = []

for df in [rag_df, rag_df2, rag_df3]:
    for _, row in df.iterrows():
        doc_text = " | ".join(str(cell) for cell in row if pd.notna(cell))
        rag_documents.append(doc_text)

rag_documents += [p.strip() for p in cure_text.split('\n\n') if p.strip()]
rag_documents += [p.strip() for p in textbook_text.split('\n\n') if p.strip()]

def get_embedding(text):
    response = openai.embeddings.create(input=text, model="nomic-embed-text")
    return np.array(response.data[0].embedding, dtype=np.float32)

document_embeddings = []
batch_size = 16

for i in range(0, len(rag_documents), batch_size):
    for txt in rag_documents[i:i + batch_size]:
        document_embeddings.append(get_embedding(txt))

document_embeddings = np.vstack(document_embeddings)
embedding_dim = document_embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)
index.add(document_embeddings)

# Transcribe function with multilingual support
async def transcribe_audio(file_path, user_lang):
    try:
        result = whisper_model.transcribe(file_path)
        transcribed_text = result["text"]

        if user_lang != "en":
            translated_text = await translate_text(transcribed_text, target_lang=user_lang)
            return translated_text  # Only return translated text (one value)
        else:
            return transcribed_text  # Only return transcribed text (one value)

    except Exception as e:
        error_msg = f"Transcription failed: {e}"
        return error_msg  # Return only the error message in case of failure

translator = Translator()

async def translate_text(text, target_lang='en'):
    result = await translator.translate(text, dest=target_lang)
    return result.text

system_message = """
You are a knowledgeable and compassionate medical AI assistant. When the user mentions any disease or medical condition, you must carefully and thoroughly analyze all five provided documents to extract accurate, detailed, and relevant information.

For each condition, provide:
1. A clear and comprehensive description of the disease, including its nature and how it affects the body.
2. Signs and symptoms the patient might experience.
3. Necessary precautions the patient should take to manage or avoid worsening the condition.
4. Recommended methods and treatments to recover from or manage the disease, based strictly on the data from the five documents.
5. Dietary recommendations and foods to be consumed or avoided, tailored to support recovery and overall health according to the disease.

Use only the information contained within the five provided documents to answer questions. If exact information is not available, infer the closest relevant information from the web.
If the user’s query is unrelated to the medical documents, gently redirect the conversation to their health concerns.
Go through all the five documents thoroughly to provide the accurate answer, the five documents have the precautions too, if the answer is not found search the web for the relevant answers.
Provide detailed answer.
"""

def retrieve_relevant_info(user_input, docs, faiss_index, k=3):
    user_emb = get_embedding(user_input)
    D, I = faiss_index.search(np.array([user_emb]), k)
    return [docs[idx] for idx in I[0] if 0 <= idx < len(docs)]

async def rag_response(transcribed_text, user_lang, selected_model):
    translated_input = await translate_text(transcribed_text, target_lang='en')
    relevant_info = retrieve_relevant_info(translated_input, rag_documents, index)
    rag_context = "\n\n".join(relevant_info)

    # Full prompt stays the same for now; format can vary by model
    full_prompt = f"""
Relevant medical document excerpts:
{rag_context}

User question:
{translated_input}

Please provide a clear, concise description, dietary recommendations, symptoms, precautions, and measures for cure based only on the above medical document.
"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": full_prompt}
    ]

    # If GitHub-hosted model
    if selected_model in AVAILABLE_MODELS:
        try:
            client = OpenAI(base_url=BASE_URL, api_key=GITHUB_TOKEN)
            response = client.chat.completions.create(
                model=selected_model,
                messages=messages,
                temperature=0.7,
                top_p=0.9
            )
            response_text = response.choices[0].message.content
        except Exception as e:
            return f"⚠️ Error from GitHub Model: {e}", None
    else:
        # Use default Ollama (local) model
        openai = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")
        response_text = ""
        stream = openai.chat.completions.create(model="llama3.2", messages=messages, stream=True)
        for chunk in stream:
            delta = chunk.choices[0].delta.content
            if delta:
                response_text += delta

    translated_response = await translate_text(response_text, target_lang=user_lang)
    audio_path = f"output_{uuid.uuid4().hex}.mp3"
    gtts_tts = gTTS(text=translated_response, lang=user_lang)
    gtts_tts.save(audio_path)

    return translated_response, audio_path


# Remove unwanted characters from text before passing to TTS
async def clean_text_for_tts(text):
    return text.replace('*', '').replace('[', '').replace(']', '')


async def handle_text_input(text, lang, selected_model):
    cleaned_text = await clean_text_for_tts(text)
    return await rag_response(cleaned_text, lang, selected_model)


with gr.Blocks() as demo:
    gr.Markdown("## 🎧 Whisper Medical Assistant")
    model_selector = gr.Dropdown(
    label="Select Generation Model",
    choices=["ollama/llama3.2"] + AVAILABLE_MODELS,
    value="ollama/llama3.2"
)

    audio_input = gr.Audio(type="filepath", label="Speak your question", interactive=True)
    text_input = gr.Textbox(label="Or Type your question", placeholder="Type here...", interactive=True)
    submit_button = gr.Button("Submit")

    transcription_output = gr.Textbox(label="Transcription Result")
    rag_output = gr.Textbox(label="Medical Response")
    audio_output = gr.Audio(label="Response Audio", type="filepath")

    selected_lang = gr.Dropdown(
        label="Select Language",
        choices=["en", "es", "fr", "de", "it", "pt", "zh", "ja", "ko", "ru", "hi", "ta", "kn", "te", "mr", "ml", "gu", "pa", "as"],
        value="en"  # Default to English
    )

    # Transcribe audio (with multilingual support)
    # Audio input handling
    audio_input.change(
    fn=transcribe_audio,
    inputs=[audio_input, selected_lang],
    outputs=[transcription_output]
).then(
    fn=rag_response,
    inputs=[transcription_output, selected_lang, model_selector],
    outputs=[rag_output, audio_output]
)

# Text input handling
    submit_button.click(
    fn=handle_text_input,
    inputs=[text_input, selected_lang, model_selector],
    outputs=[rag_output, audio_output]
)


demo.launch(inbrowser=True, debug=True, share=True)

